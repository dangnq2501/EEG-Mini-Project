{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9810623,"sourceType":"datasetVersion","datasetId":5998417}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(dirname)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:18:48.218837Z","iopub.execute_input":"2024-12-15T04:18:48.219220Z","iopub.status.idle":"2024-12-15T04:18:48.639196Z","shell.execute_reply.started":"2024-12-15T04:18:48.219185Z","shell.execute_reply":"2024-12-15T04:18:48.638065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install mne\n!pip install optuna pytorch_lightning\n!pip install optuna.integration[pytorch_lightning]`","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:18:48.641417Z","iopub.execute_input":"2024-12-15T04:18:48.641887Z","iopub.status.idle":"2024-12-15T04:19:10.957265Z","shell.execute_reply.started":"2024-12-15T04:18:48.641851Z","shell.execute_reply":"2024-12-15T04:19:10.955826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import mne\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfile_paths = [f\"/kaggle/input/BCICIV_2a_gdf/A0{i}T.gdf\" for i in range(1, 10)]  \n\nall_data = []\nall_labels = []\n\nevent_ids = {\n    'left_hand': 7,   \n    'right_hand': 8,   \n    'feet': 9,         \n    'tongue': 10       \n}\n\nfor file_path in file_paths:\n    raw = mne.io.read_raw_gdf(file_path, preload=True)\n    raw.notch_filter(freqs=50)# Notch filter\n    raw.filter(8., 30., fir_design='firwin')#Band-Pass Filter to Retain Frequencies of Interest\n    raw.set_eeg_reference('average')# Re-reference the EEG Signals\n    \n    events, event_dict = mne.events_from_annotations(raw)\n    print(f\"Available events in {file_path}: {event_dict}\")\n    \n    available_event_ids = {key: event_ids[key] for key in event_ids if event_ids[key] in event_dict.values()}\n    \n    if not available_event_ids:\n        print(f\"No motor imagery tasks found in {file_path}. Skipping this file.\")\n        continue  \n    \n    epochs = mne.Epochs(raw, events, event_id=available_event_ids, tmin=0, tmax=4, baseline=None, preload=True)\n    \n    X = epochs.get_data() \n    X_normalized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n    y = epochs.events[:, -1] - min(available_event_ids.values())  \n    \n    all_data.append(X_normalized)\n    all_labels.append(y)\n\nX_combined = np.concatenate(all_data, axis=0) \ny_combined = np.concatenate(all_labels, axis=0)  \n\nX_tensor = torch.tensor(X_combined, dtype=torch.float32).unsqueeze(1) \ny_tensor = torch.tensor(y_combined, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:19:10.959095Z","iopub.execute_input":"2024-12-15T04:19:10.959484Z","iopub.status.idle":"2024-12-15T04:19:47.978077Z","shell.execute_reply.started":"2024-12-15T04:19:10.959447Z","shell.execute_reply":"2024-12-15T04:19:47.977014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y_tensor)\n\n\ntrain_dataset = TensorDataset(X_train, y_train)\nval_dataset = TensorDataset(X_val, y_val)\nbatch_size=64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n\n\nclass EEGAutoencoder(nn.Module):\n    def __init__(self, n_channels=22, n_times=501, latent_dim=64):\n        super(EEGAutoencoder, self).__init__()\n        self.n_channels = n_channels\n        self.n_times = n_times\n        input_dim = n_channels * n_times\n        \n        self.encoder = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, latent_dim)\n        )\n        \n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, input_dim),\n        )\n        \n    def forward(self, x):\n        z = self.encoder(x)\n        x_recon = self.decoder(z)\n        x_recon = x_recon.view(-1, 1, self.n_channels, self.n_times)\n        return x_recon, z\n\nn_channels = X_tensor.shape[2]\nn_times = X_tensor.shape[3]\nlatent_dim = 128\nae = EEGAutoencoder(n_channels=n_channels, n_times=n_times, latent_dim=latent_dim)\nae_optimizer = optim.Adam(ae.parameters(), lr=1e-3)\nae_criterion = nn.MSELoss()\n\n\nnum_epochs_ae = 50\nfor epoch in range(num_epochs_ae):\n    ae.train()\n    total_loss = 0.0\n    for X_batch, _ in train_loader:\n        ae_optimizer.zero_grad()\n        X_recon, _ = ae(X_batch)\n        loss = ae_criterion(X_recon, X_batch)\n        loss.backward()\n        ae_optimizer.step()\n        total_loss += loss.item()\n    print(f\"AE Epoch {epoch+1}/{num_epochs_ae}, Loss: {total_loss/len(train_loader):.4f}\")\n\nae.eval()\nwith torch.no_grad():\n    train_features = []\n    train_labels = []\n    for X_batch, y_batch in train_loader:\n        _, z = ae(X_batch)\n        train_features.append(z)\n        train_labels.append(y_batch)\n    train_features = torch.cat(train_features, dim=0)\n    train_labels = torch.cat(train_labels, dim=0)\n    \n    val_features = []\n    val_labels = []\n    for X_batch, y_batch in val_loader:\n        _, z = ae(X_batch)\n        val_features.append(z)\n        val_labels.append(y_batch)\n    val_features = torch.cat(val_features, dim=0)\n    val_labels = torch.cat(val_labels, dim=0)\n\n\ntrain_feat_dataset = TensorDataset(train_features, train_labels)\nval_feat_dataset = TensorDataset(val_features, val_labels)\nbatch_size = 64\ntrain_feat_loader = DataLoader(train_feat_dataset, batch_size=batch_size, shuffle=True)\nval_feat_loader = DataLoader(val_feat_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:19:47.979694Z","iopub.execute_input":"2024-12-15T04:19:47.980348Z","iopub.status.idle":"2024-12-15T04:24:02.212954Z","shell.execute_reply.started":"2024-12-15T04:19:47.980310Z","shell.execute_reply":"2024-12-15T04:24:02.211954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for X, y in train_loader:\n    print(X.shape, y.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:24:02.216554Z","iopub.execute_input":"2024-12-15T04:24:02.217217Z","iopub.status.idle":"2024-12-15T04:24:02.226592Z","shell.execute_reply.started":"2024-12-15T04:24:02.217180Z","shell.execute_reply":"2024-12-15T04:24:02.225467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EEGNet(nn.Module):\n    def __init__(self, \n                 num_classes, \n                 Chans=25, \n                 Samples=1001, \n                 dropoutRate=0.5, \n                 kernLength=64, \n                 F1=8, \n                 D=2, \n                 F2=16):\n        super(EEGNet, self).__init__()\n\n        self.firstconv = nn.Sequential(\n            nn.Conv2d(1, F1, kernel_size=(1, kernLength), padding=(0, kernLength // 2), bias=False),\n            nn.BatchNorm2d(F1)\n        )\n\n        self.depthwiseConv = nn.Sequential(\n            nn.Conv2d(F1, F1 * D, kernel_size=(Chans, 1), groups=F1, bias=False),\n            nn.BatchNorm2d(F1 * D),\n            nn.ELU(),\n            nn.AvgPool2d(kernel_size=(1, 4)),\n            nn.Dropout(p=dropoutRate)\n        )\n\n        self.separableConv = nn.Sequential(\n            nn.Conv2d(F1 * D, F1 * D, kernel_size=(1,16), padding=(0,16//2), groups=F1*D, bias=False),\n            nn.Conv2d(F1 * D, F2, kernel_size=(1,1), bias=False),\n            nn.BatchNorm2d(F2),\n            nn.ELU(),\n            nn.AvgPool2d(kernel_size=(1,8)),\n            nn.Dropout(p=dropoutRate)\n        )\n\n        self.flattened_size = self._get_flattened_size(Chans, Samples)\n        self.classify = nn.Linear(self.flattened_size, num_classes)\n\n    def _get_flattened_size(self, Chans, Samples):\n        with torch.no_grad():\n            x = torch.zeros(1, 1, Chans, Samples)\n            x = self.firstconv(x)\n            x = self.depthwiseConv(x)\n            x = self.separableConv(x)\n            # print(\"Predict: \", x.shape)\n            return x.shape[1] * x.shape[2] * x.shape[3]\n\n    def forward_features(self, x):\n        x = self.firstconv(x)\n        x = self.depthwiseConv(x)\n        x = self.separableConv(x)\n        x = x.view(x.size(0), -1)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        # print(\"True: \", x.shape)\n        x = self.classify(x)\n        return x\n\n\nclass EEGNetLightning(pl.LightningModule):\n    def __init__(self, num_classes=4, Chans=25, Samples=501, lr=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = EEGNet(num_classes=num_classes, Chans=Chans, Samples=Samples)\n        self.criterion = nn.CrossEntropyLoss()\n\n    def training_step(self, batch, batch_idx):\n        X_batch, y_batch = batch\n        logits = self.model(X_batch)\n        loss = self.criterion(logits, y_batch)\n        self.log('train_loss', loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        X_batch, y_batch = batch\n        logits = self.model(X_batch)\n        loss = self.criterion(logits, y_batch)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y_batch).float().mean()\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc, prog_bar=True, on_epoch=True)\n        return loss\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.hparams.lr)\n\ntrainer = pl.Trainer(max_epochs=30)\nmodel = EEGNetLightning(num_classes=4, Chans=25, Samples=1001, lr=1e-3)\ntrainer.fit(model, train_loader, val_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:24:02.227949Z","iopub.execute_input":"2024-12-15T04:24:02.228291Z","iopub.status.idle":"2024-12-15T04:31:52.641997Z","shell.execute_reply.started":"2024-12-15T04:24:02.228259Z","shell.execute_reply":"2024-12-15T04:31:52.640745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef eval_confusion_matrix(model, val_loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for X_batch, y_batch in val_loader:\n            outputs = model.model(X_batch)\n            _, preds = torch.max(outputs, 1)\n    \n            all_preds.append(preds.cpu().numpy())\n            all_labels.append(y_batch.cpu().numpy())\n    \n    all_preds = np.concatenate(all_preds)\n    all_labels = np.concatenate(all_labels)\n    \n    test_accuracy = accuracy_score(all_labels, all_preds)\n    print(f'Test Accuracy: {test_accuracy:.4f}')\n    \n    print(classification_report(all_labels, all_preds))\n    \n    cm = confusion_matrix(all_labels, all_preds)\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    class_names = ['Left Hand', 'Right Hand', 'Feet', 'Tongue']  # Thay thế bằng tên lớp của bạn\n    \n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix (Normalized)')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.show()\neval_confusion_matrix(model, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:31:52.643518Z","iopub.execute_input":"2024-12-15T04:31:52.643835Z","iopub.status.idle":"2024-12-15T04:31:54.613804Z","shell.execute_reply.started":"2024-12-15T04:31:52.643804Z","shell.execute_reply":"2024-12-15T04:31:54.612625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport optuna\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:31:54.615114Z","iopub.execute_input":"2024-12-15T04:31:54.615828Z","iopub.status.idle":"2024-12-15T04:31:54.746591Z","shell.execute_reply.started":"2024-12-15T04:31:54.615791Z","shell.execute_reply":"2024-12-15T04:31:54.745614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiBranchEEGNetAE(pl.LightningModule):\n    def __init__(self, \n                 num_classes=4, \n                 Chans=25, \n                 Samples=1001, \n                 latent_dim=64, \n                 hidden_dim=64, \n                 dropout_rate=0.3, \n                 lr=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.eegnet = EEGNet(num_classes=num_classes, Chans=Chans, Samples=Samples)\n        eegnet_features_dim = self.eegnet.flattened_size\n\n        # AE branch\n        self.ae_branch = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate)\n        )\n\n        # Final classifier after concatenation of EEGNet features and AE features\n        self.classifier = nn.Sequential(\n            nn.Linear(eegnet_features_dim + hidden_dim, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(64, num_classes)\n        )\n\n        self.criterion = nn.CrossEntropyLoss()\n\n    def forward(self, X_raw, Z_ae):\n        # EEGNet features\n        eegnet_feats = self.eegnet.forward_features(X_raw)  # [B, eegnet_features_dim]\n        ae_feats = self.ae_branch(Z_ae)                     # [B, hidden_dim]\n        out = torch.cat((eegnet_feats, ae_feats), dim=1)\n        logits = self.classifier(out)\n        return logits\n\n    def training_step(self, batch, batch_idx):\n        X_raw, Z_ae, y_batch = batch\n        logits = self(X_raw, Z_ae)\n        loss = self.criterion(logits, y_batch)\n        self.log('train_loss', loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        X_raw, Z_ae, y_batch = batch\n        logits = self(X_raw, Z_ae)\n        loss = self.criterion(logits, y_batch)\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y_batch).float().mean()\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc, prog_bar=True, on_epoch=True)\n        return loss\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.hparams.lr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:31:54.748094Z","iopub.execute_input":"2024-12-15T04:31:54.748567Z","iopub.status.idle":"2024-12-15T04:31:54.760979Z","shell.execute_reply.started":"2024-12-15T04:31:54.748516Z","shell.execute_reply":"2024-12-15T04:31:54.759700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CombinedDataset(torch.utils.data.Dataset):\n    def __init__(self, raw_data, ae_features, labels):\n        self.raw_data = raw_data\n        self.ae_features = ae_features\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.raw_data[idx], self.ae_features[idx], self.labels[idx]\n\ntrain_combined_dataset = CombinedDataset(X_train, train_features, y_train)\nval_combined_dataset = CombinedDataset(X_val, val_features, y_val)\n\ntrain_combined_loader = DataLoader(train_combined_dataset, batch_size=batch_size, shuffle=True)\nval_combined_loader = DataLoader(val_combined_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:31:54.762565Z","iopub.execute_input":"2024-12-15T04:31:54.762898Z","iopub.status.idle":"2024-12-15T04:31:54.782155Z","shell.execute_reply.started":"2024-12-15T04:31:54.762868Z","shell.execute_reply":"2024-12-15T04:31:54.780925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_ae_eegnet = MultiBranchEEGNetAE(num_classes=4, \n                                      Chans=25, \n                                      Samples=1001, \n                                      latent_dim=latent_dim, \n                                      hidden_dim=64, \n                                      dropout_rate=0.3, \n                                      lr=1e-3)\n\ntrainer = pl.Trainer(max_epochs=60)\ntrainer.fit(model_ae_eegnet, train_combined_loader, val_combined_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:31:54.783658Z","iopub.execute_input":"2024-12-15T04:31:54.784114Z","iopub.status.idle":"2024-12-15T04:47:07.679234Z","shell.execute_reply.started":"2024-12-15T04:31:54.784069Z","shell.execute_reply":"2024-12-15T04:47:07.678203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def combined_eval_confusion_matrix(model, val_loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for X_batch, Z_batch, y_batch in val_loader:\n            outputs = model_ae_eegnet(X_batch, Z_batch)\n            _, preds = torch.max(outputs, 1)\n    \n            all_preds.append(preds.cpu().numpy())\n            all_labels.append(y_batch.cpu().numpy())\n    \n    all_preds = np.concatenate(all_preds)\n    all_labels = np.concatenate(all_labels)\n    \n    test_accuracy = accuracy_score(all_labels, all_preds)\n    print(f'Test Accuracy: {test_accuracy:.4f}')\n    \n    print(classification_report(all_labels, all_preds))\n    \n    cm = confusion_matrix(all_labels, all_preds)\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    class_names = ['Left Hand', 'Right Hand', 'Feet', 'Tongue']  # Thay thế bằng tên lớp của bạn\n    \n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix (Normalized)')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.show()\ncombined_eval_confusion_matrix(model_ae_eegnet, val_combined_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:47:07.680646Z","iopub.execute_input":"2024-12-15T04:47:07.680993Z","iopub.status.idle":"2024-12-15T04:47:09.391303Z","shell.execute_reply.started":"2024-12-15T04:47:07.680959Z","shell.execute_reply":"2024-12-15T04:47:09.390297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial):\n    hidden_dim = trial.suggest_int('hidden_dim', 32, 256, step=32)\n    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5, step=0.1)\n    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n\n    model_ae_eegnet = MultiBranchEEGNetAE(num_classes=4, \n                                          Chans=25, \n                                          Samples=1001, \n                                          latent_dim=latent_dim, \n                                          hidden_dim=hidden_dim, \n                                          dropout_rate=dropout_rate, \n                                          lr=lr)\n\n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_acc',\n        mode='max',\n        save_top_k=1,\n        filename='best-checkpoint-{epoch:02d}-{val_acc:.2f}'\n    )\n\n    trainer = pl.Trainer(\n        max_epochs=60,\n        callbacks=[checkpoint_callback],\n        enable_progress_bar=False,\n        log_every_n_steps=1\n    )\n\n    trainer.fit(model_ae_eegnet, train_combined_loader, val_combined_loader)\n    return trainer.callback_metrics['val_acc'].item()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(f\"  Value: {trial.value}\")\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\nbest_params = study.best_params\nbest_model = MultiBranchEEGNetAE(num_classes=4, \n                                    Chans=25, \n                                    Samples=1001, \n                                    latent_dim=latent_dim, \n                                    hidden_dim=best_params['hidden_dim'], \n                                    dropout_rate=best_params['dropout_rate'], \n                                    lr=best_params['lr'])\ntrainer = pl.Trainer(\n    max_epochs=50,\n    callbacks=[checkpoint_callback],\n    enable_progress_bar=False,\n    log_every_n_steps=1\n)\n\ntrainer.fit(best_model, train_combined_loader, val_combined_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:47:09.392964Z","iopub.execute_input":"2024-12-15T04:47:09.393435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ncombined_eval_confusion_matrix(best_model, val_combined_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}